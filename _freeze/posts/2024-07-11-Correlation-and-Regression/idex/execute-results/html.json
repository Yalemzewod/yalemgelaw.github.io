{
  "hash": "23314b0538ef140e41306872ef629180",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Correlation and Linear Regression using R\"\ndescription: |\n  Exploring Correlation and Linear Regression with R: A Comprehensive Guide\nauthor: \n  - name: Yalemzewod Gelaw\n    url: https://yalemzewodgelaw.com/\n    orcid: 0000-0002-5338-586 \ndate: 2024-07-11\nimage: \"media/cor_reg.png\"\nslug: beginner’s guide\ncategories: [RStudio, R]\ntoc: true\ntoc-depth: 4\nnumber-sections: false\nhighlight-style: github\nformat:\n  html:\n    self-contained: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-tools: true\n    theme: united \nknitr: \n opts_knit: \n   warning: false\n   message: false\neditor: visual\n---\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install necessary packages if not already installed\nif (!requireNamespace(\"tidyverse\", quietly = TRUE)) install.packages(\"tidyverse\")\n\nif (!requireNamespace(\"janitor\", quietly = TRUE)) install.packages(\"janitor\")\n\nif (!requireNamespace(\"car\", quietly = TRUE)) install.packages(\"car\")\n\nif (!requireNamespace(\"Hmisc\", quietly = TRUE)) install.packages(\"Hmisc\")\n\nif (!requireNamespace(\"corrplot\", quietly = TRUE)) install.packages(\"corrplot\")\n\nif (!requireNamespace(\"MASS\", quietly = TRUE)) install.packages(\"MASS\")\n\nif (!requireNamespace(\"ggpubr\", quietly = TRUE)) install.packages(\"ggpubr\")\n\nif (!requireNamespace(\"ggcorrplot\", quietly = TRUE)) install.packages(\"ggcorrplot\")\n\nif (!requireNamespace(\"olsrr\", quietly = TRUE)) install.packages(\"olsrr\")\n\nif (!requireNamespace(\"caret\", quietly = TRUE)) install.packages(\"caret\")\n\n# Load the packages\nlibrary(tidyverse)   # For data wrganling \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyverse' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggplot2' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tibble' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'tidyr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'readr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'forcats' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'lubridate' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(janitor)     # To clean column names\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'janitor' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(Hmisc)       # To create a correlation coeficient \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Hmisc' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Hmisc'\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(car)         # Independent test\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'car' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: carData\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'carData' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)    # To visualise correlation matrix\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'corrplot' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\ncorrplot 0.92 loaded\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(MASS)        # Create a hsitogram\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggpubr)      # customizing the plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggpubr' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggcorrplot)  # correlation matrix\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'ggcorrplot' was built under R version 4.3.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(olsrr)       # variable selection\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'olsrr' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'olsrr'\n\nThe following object is masked from 'package:MASS':\n\n    cement\n\nThe following object is masked from 'package:datasets':\n\n    rivers\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(caret)       # cross validation\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'caret' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n\n\n:::\n:::\n\n\n### **About the data**\n\nThe dataset used in this analysis comes from a study of 94 participants performing treadmill exercises. Data were collected on the following variables: Maximum Volume of Oxygen used per minute per kilogram of body weight `(max VO2)`, duration of exercise `(duration)`, maximum heart rate during exercise `(heart rate)`, `age`, `height`, `weight`, `sex` (1 = male, 2 = female), `ethnicity` (1 = Habesha, 2 = White, 3 = West African), and whether there was an energy shortage during `exercise` (energy; 1 = yes, 0 = no).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexcercise <- read_csv(\"Data/excercise.csv\") %>% \n  clean_names()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 94 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): duration, max VO2, HeartRate, age, height, weight, ethnicity, sex, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n### **Exploratory data analysis**\n\nLets investigate the relationships among the continuous variables in our dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Summary of continous variable\nsummary(excercise)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    duration        max_vo2        heart_rate         age           height     \n Min.   :180.0   Min.   :15.30   Min.   :104.0   Min.   :21.0   Min.   :154.0  \n 1st Qu.:520.8   1st Qu.:30.40   1st Qu.:168.0   1st Qu.:44.0   1st Qu.:168.0  \n Median :599.5   Median :35.85   Median :175.0   Median :49.5   Median :174.0  \n Mean   :576.7   Mean   :35.63   Mean   :175.4   Mean   :49.6   Mean   :173.9  \n 3rd Qu.:647.8   3rd Qu.:41.38   3rd Qu.:188.0   3rd Qu.:57.0   3rd Qu.:180.0  \n Max.   :827.0   Max.   :50.90   Max.   :210.0   Max.   :74.0   Max.   :193.0  \n     weight         ethnicity         sex        energy_shortage \n Min.   : 47.00   Min.   :1.00   Min.   :1.000   Min.   :0.0000  \n 1st Qu.: 65.25   1st Qu.:1.25   1st Qu.:1.000   1st Qu.:0.0000  \n Median : 76.50   Median :2.00   Median :1.000   Median :0.0000  \n Mean   : 73.31   Mean   :2.17   Mean   :1.362   Mean   :0.4362  \n 3rd Qu.: 82.00   3rd Qu.:3.00   3rd Qu.:2.000   3rd Qu.:1.0000  \n Max.   :106.00   Max.   :3.00   Max.   :2.000   Max.   :1.0000  \n```\n\n\n:::\n:::\n\n\nTo better understand the data, we will label the numeric categories for sex, ethnicity, and energy shortage as factors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexcercise <- excercise %>%\n  mutate(\n    sex = if_else(sex == 1, \"male\",\n                  if_else(sex == 2, \"female\", NA)),\n    ethnicity = if_else(ethnicity == 1, \"Habesha\",\n                        if_else(ethnicity == 2, \"White\",\n                                if_else(ethnicity == 3, \"West African\", NA))),\n    energy = if_else(energy_shortage == 1, \"yes\",\n                     if_else(energy_shortage==0, \"no\", NA)),\n  BMI = round(weight/(height/100)^2)\n  ) %>% \n  dplyr::select(-energy_shortage)\n```\n:::\n\n\n#### **Correlation Analysis**\n\nCorrelation measures the strength and direction of the linear relationship between two variables. We'll explore Pearson’s, Spearman’s, and Kendall’s correlation coefficients.\n\n#### Correlation Matrix\n\nWe'll start by creating a correlation matrix for the continuous variables using the `cor()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select continuous variables\ncont_var <- excercise %>% \n  dplyr::select(duration:weight, BMI)\n\n# Create correlation matrix\ncor(cont_var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             duration     max_vo2   heart_rate        age      height\nduration    1.0000000  0.90639686  0.677786803 -0.6570770 -0.07834020\nmax_vo2     0.9063969  1.00000000  0.647399860 -0.6247552 -0.04562266\nheart_rate  0.6777868  0.64739986  1.000000000 -0.5859920  0.09109966\nage        -0.6570770 -0.62475521 -0.585992041  1.0000000 -0.14772007\nheight     -0.0783402 -0.04562266  0.091099661 -0.1477201  1.00000000\nweight     -0.1789527 -0.16159781 -0.006629393 -0.1834303  0.76228720\nBMI        -0.1875713 -0.19115742 -0.078175596 -0.1400085  0.34483241\n                 weight        BMI\nduration   -0.178952680 -0.1875713\nmax_vo2    -0.161597810 -0.1911574\nheart_rate -0.006629393 -0.0781756\nage        -0.183430258 -0.1400085\nheight      0.762287200  0.3448324\nweight      1.000000000  0.8633786\nBMI         0.863378561  1.0000000\n```\n\n\n:::\n:::\n\n\n**The `rcorr ()` Function**\n\nIt will create a correlation matric with p value. The first matrix shows the correlation coefficients between the variables and the second matrix shows the corresponding p-values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create matrix of correlation coefficients and p-values\nrcorr(as.matrix(cont_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           duration max_vo2 heart_rate   age height weight   BMI\nduration       1.00    0.91       0.68 -0.66  -0.08  -0.18 -0.19\nmax_vo2        0.91    1.00       0.65 -0.62  -0.05  -0.16 -0.19\nheart_rate     0.68    0.65       1.00 -0.59   0.09  -0.01 -0.08\nage           -0.66   -0.62      -0.59  1.00  -0.15  -0.18 -0.14\nheight        -0.08   -0.05       0.09 -0.15   1.00   0.76  0.34\nweight        -0.18   -0.16      -0.01 -0.18   0.76   1.00  0.86\nBMI           -0.19   -0.19      -0.08 -0.14   0.34   0.86  1.00\n\nn= 94 \n\n\nP\n           duration max_vo2 heart_rate age    height weight BMI   \nduration            0.0000  0.0000     0.0000 0.4529 0.0844 0.0702\nmax_vo2    0.0000           0.0000     0.0000 0.6624 0.1197 0.0650\nheart_rate 0.0000   0.0000             0.0000 0.3825 0.9494 0.4539\nage        0.0000   0.0000  0.0000            0.1554 0.0768 0.1783\nheight     0.4529   0.6624  0.3825     0.1554        0.0000 0.0007\nweight     0.0844   0.1197  0.9494     0.0768 0.0000        0.0000\nBMI        0.0702   0.0650  0.4539     0.1783 0.0007 0.0000       \n```\n\n\n:::\n:::\n\n\n#### Visualizing Correlation\n\nWe can visualize the correlation matrix using the `corrplot` function from the `corrplot` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#visualize correlation matrix\ncorrplot(cor(cont_var))\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/corplot-1.png){width=672}\n:::\n:::\n\n\n#### Correlation Coefficients\n\n##### Pearson Correlation\n\nPearson’s correlation measures the linear relationship between two continuous variables. It ranges from -1 to 1, where 1 means a perfect positive linear relationship, -1 means a perfect negative linear relationship, and 0 means no linear relationship.\n\nTo calculate Pearson’s correlation coefficient between the `max_vo2` and `duartion` variables in the `exercise` dataset, use the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Pearson's correlation between max_vo2 and duration\ncor(excercise$max_vo2, excercise$duration, method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9063969\n```\n\n\n:::\n:::\n\n\n##### **Spearman’s Correlation for Ranked Variables**\n\nSpearman’s correlation measures the monotonic relationship between two ranked variables. It is suitable for ordinal data or continuous data that do not meet the assumptions of Pearson’s correlation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(excercise$max_vo2, excercise$heart_rate, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5390491\n```\n\n\n:::\n:::\n\n\n##### Kendall’s Correlation\n\nKendall’s correlation coefficient is used when the sample size is small and there are many tied ranks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Spearman's correlation between max_vo2 and heart_rate\ncor(excercise$max_vo2, excercise$age, method = \"kendall\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.4221638\n```\n\n\n:::\n:::\n\n\n**Properties of correlation coefficient (r) are:**\n\n-   `r` only measures the strength of a linear relationship.\n\n-   `r` is always between -1 and 1 inclusive. -1 means perfect negative linear correlation and +1 means perfect positive linear correlation\n\n-   `r` does not change if the independent (x) and dependent (y) variables are inter- changed\n\n-   `r` does not change if the scale on either variable is changed. You may multiply, divide, add, or subtract a value from all the x-values or y-values without changing the value of r.\n\n##### Scatter Plot\n\nLet's create a scatter plot to visualize the relationship between `max_vo2` and `duration`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n # Scatter plot\nexcercise %>% \n  ggplot() +\n  # Add color\n  # Add shape\n  # Add transparency\n  geom_point(aes(x=duration, y=max_vo2,\n                 colour = sex),\n             alpha = 1/2) +\n  # Add main title, axis labels, and legend titles\n  labs(title = \"Correlation between maximum volume of oxygen and durations \\n doing exercise on treadmill (n =94)\", \n       subtitle = \"Source: Online\",\n       y = \"Max Vo2 (minutes/kg)\", \n       x = \"Duration (minutes)\",\n       colour=\"Sex\") +\n  # Customize theme\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/scatter-1.png){width=672}\n:::\n:::\n\n\n*Figure 1: Scatter plot - the relationship between maximum volume of oxygen and duration of exercise.*\n\nAs we can see the scatter plot displays the form and directions of the relationship between maximum volume of oxygen and duration of exercise - the maximum volume of oxygen increase when the duration of exercise increased.\n\n### Linear Regression\n\nLinear regression models the relationship between two variables by fitting a linear equation to observed data. The explanatory variable (x) and the dependent variable (y) are key components, with Y being numeric (continuous). In Figure 1, the relationship between the maximum volume of oxygen (Y) and duration of exercise (X) the line that best predicts `max_vo2` from `duration` by minimizing the sum of the squares of the vertical distances of the points from the line.\n\n::: callout-note\n#### Assumptions of Linear Regression\n\n1.  **Independence of Observations**: Residuals should not be correlated.\n\n2.  **Linearity**: The relationship between the independent and dependent variables should be linear.\n\n3.  **Homoscedasticity**: Residuals should have constant variance.\n\n4.  **Normality**: Residuals should follow a normal distribution\n:::\n\n##### **1. Independence of Observations (No Autocorrelation)**\n\nThe observations must be independent of each other, meaning the residuals (errors) from one observation should not correlate with the residuals of another. This assumption could be check using the [Durbin-Watson test](https://www.statology.org/durbin-watson-test/). We will check after we fit a model. A significant test statistics corresponding p-value depict the residuals in the regression model is autocorrelated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fitting the model\nmodel = lm(max_vo2 ~ duration, data = excercise) \n#perform Durbin-Watson test\ndurbinWatsonTest(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n lag Autocorrelation D-W Statistic p-value\n   1    -0.007425459      1.976968    0.87\n Alternative hypothesis: rho != 0\n```\n\n\n:::\n:::\n\n\nFrom the output above we can see that the p-value is 0.866 which is indicating that the residuals in this regression model are not autocorrelated.\n\n##### 2. Linearity\n\nThe relationship between the independent (duration) and dependent (max_vo2) variables must be linear. This can be assessed by plotting the variables and looking for a linear pattern.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plotting to check linearity\nplot(max_vo2 ~ duration, data = excercise)  \n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/linear-1.png){width=672}\n:::\n:::\n\n\nThe points in the plot above look like they fall on roughly a straight line, which indicates that there is a linear relationship between max_vo2 and duration in exercise.\n\n##### **3. Homoscedasticity**\n\nThe equal variance assumptions is that the residuals have constant variance at every level of x. This means that the prediction error doesn’t change significantly over the range of prediction of the model. The assumptions will be evaluated by creating a fitted value vs. residual plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nplot(model)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/homosc-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n```\n:::\n\n\nThe scatter plot shows the residual are closer the fitted values.\n\n##### 4. Normality\n\nThe residuals (errors) should follow a normal distribution. This assumption can be checked by plotting a histogram of the residuals, Q_Q plots.\n\nExample of histogram:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntruehist(excercise$max_vo2, xlab = \"Max_VO2\", ylab = \"Percentage\", main= \"Max_VO2 per minute per kilogram of body weight~Normal\", prob = TRUE)\ncurve (dnorm(x,mean=mean(excercise$max_vo2),sd=sd(excercise$max_vo2)), add=TRUE)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/normality-1.png){width=672}\n:::\n:::\n\n\n***Figure 3**: Histogram (with fitted Normal distribution) of Max_VO2(minutes/Kg)*\n\nThe distribution (**Figure3**) extends from 20 to 50; a possible low and high values of the volume of oxygen, forcibly blown out in one second after a full intake of breath. The distribution seems asymmetric (skewed to the left).\n\nExample of Q-Q plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqnorm(excercise$max_vo2)\nqqline(excercise$max_vo2,\n       main = 'Q-Q Plot for Normality', xlab = 'Theoretical Dist',\n       ylab = 'Sample dist', col = 'steelblue')\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/q-q plot-1.png){width=672}\n:::\n:::\n\n\nOR , we can plot from the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model, 2)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/plt-1.png){width=672}\n:::\n:::\n\n\n**Shapiro-Wilk Test** is a common statistical test of normality. The test provides a test statistic *W* along with a corresponding *p-value*. If the *p-value* is less than *α =.05*, there is sufficient evidence to say that the sample does not come from a population that is normally distributed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(excercise$max_vo2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  excercise$max_vo2\nW = 0.97977, p-value = 0.1539\n```\n\n\n:::\n:::\n\n\nThe *Shapiro-Wilk Normality test* tells us the maximum volume of oxygen is distributed normally.\n\nOur assumptions check showed as all the assumptions are met.\n\n**How linear regression works?**\n\nThe most common method for fitting a linear regression line is the method of least-squares. This method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). More precisely, the goal of regression is to minimize the sum of the squares of the vertical distances of the points from the line.\n\n#### Simple Linear Regression\n\nA regression model aims to model the magnitude of the relationship between one independent variable and a dependent variable. The simple linear regression model can be written with an explicit error term as:\n\n$$\ny= a+ b_1 (x) +e\n$$\n\nWhere $a$ is the intercept of the regression line when the $x$ is null; $b_1$ is the slope of the regression line. and $e$ is a random component. It is the difference between the actual and the estimated dependent variable based on the size of the independent variable. The value of error (e) will vary from subject to subject, even if independent variable remains the same. Note that both α and b are population parameters which are usually unknown and hence estimated from the data by a and b.\n\nBelow is the equation using our data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression model\nmodel<- lm(max_vo2 ~ duration, data = excercise) \n```\n:::\n\n\nThe model results will be printed using the summary function\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# model output\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = max_vo2 ~ duration, data = excercise)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8341 -1.7243  0.0338  1.8036  7.7629 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.06473    1.56876   2.591   0.0111 *  \nduration     0.05474    0.00266  20.581   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.19 on 92 degrees of freedom\nMultiple R-squared:  0.8216,\tAdjusted R-squared:  0.8196 \nF-statistic: 423.6 on 1 and 92 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n*Estimated*: the coefficient for duration\n\n*R-square*: the proportion of variance explained by the model and it measures the strength of the relationship between the model and the dependent variable.\n\n*p-value and t-statistic*: they measure the extent to which a given coefficient is statistically significant - a higher t-statistics corresponds to a smaller p-value which indicates strong evidence against the null hypothesis.\n\nTo get the confidence interval we can use the `confint()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confidence interval\nconfint.lm(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %     97.5 %\n(Intercept) 0.94903418 7.18041819\nduration    0.04945863 0.06002395\n```\n\n\n:::\n:::\n\n\nWe can also plot the residuals with histogram to check the normality assumptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nResiduals <- residuals(model)\nhist(Residuals, freq = FALSE)\ncurve(dnorm(x,mean = mean(Residuals),sd=sd(Residuals)), add = TRUE)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/resid-1.png){width=672}\n:::\n:::\n\n\n**Interpreting the results:**\n\nThe fitted model of max_vo2 by duaration (minutes):\n\n$$\nmax_vo2 = 4.1 + 0.055 * durations (minutes)\n$$\n\nA minutes increase in duration is associated with an expected increase of 0.055 minutes/kg in max_vo2.\n\nThe fitted model can be visualized with a regression line added to the scatter plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc.graph<-ggplot(model,\n                  aes(x=duration,\n                      y=max_vo2 ))+\n  geom_point() +\n  labs(title = \"The fitted model of max_vo2 by duration\",\n       y = \"Max_vo2 (minutes/kg)\",\n       x = \"Durations (minutes)\") +\n  theme_bw()\n# print\ninc.graph\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/result_graph-1.png){width=672}\n:::\n:::\n\n\nAdd the linear regression line to the plotted data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc.graph <- inc.graph + geom_smooth(method=\"lm\", col=\"red\")\n\n# print\ninc.graph\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](idex_files/figure-html/add_line-1.png){width=672}\n:::\n:::\n\n\nAdd the equation for the regression line.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninc.graph <- inc.graph +\n  stat_regline_equation(label.x = 205, label.y = 27) + # for regression equation\n  stat_cor(aes(label = after_stat(rr.label)), abel.x = 205, label.y = 24) # for R^2\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in stat_cor(aes(label = after_stat(rr.label)), abel.x = 205, label.y =\n24): Ignoring unknown parameters: `abel.x`\n```\n\n\n:::\n\n```{.r .cell-code}\ninc.graph\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](idex_files/figure-html/add_equation-1.png){width=672}\n:::\n:::\n\n\n#### Multiple Linear Regression\n\nMultiple linear regression model aims to model the magnitude of the relationship between two variable while taking account the effect of other explantory variable and a dependent variable.\n\n$$\ny = a + bx_1 + bx_2 + bx_3 + bx_n + e\n$$\n\n***Multicollinearity*** - the dependency between predictors is another assumptions of linear regression analysis when we fit multiple regression analysis.\n\nThe presence of multidisciplinary can be checked if there is a an inflated estimated coefficients and untrue relationships.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute correlation at 2 decimal places\ncorr_matrix = round(cor(cont_var), 2)\n\n# Compute and show the  result\nggcorrplot(corr_matrix, hc.order = TRUE, type = \"lower\",\n          lab = TRUE)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/cor_matrix-1.png){width=672}\n:::\n:::\n\n\nWe can notice two strong correlations because their value is higher than 0.8.\n\n-          Max_vo2 and duration: 0.91\n\n-          BMI and weight: 0.86\n\nThis result makes sense because BMI is computed from weight and age. In this case, we can get rid of the either BMI or weight from our model.\n\nLet's illustrate this by adding more variable into the previous model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit multiple linear regression model\nmodel2 <- lm(lm(formula = max_vo2 ~., data = excercise))\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = lm(formula = max_vo2 ~ ., data = excercise))\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.647 -1.653  0.298  1.527  5.388 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           26.265707  30.979448   0.848    0.399    \nduration               0.039669   0.004217   9.406 1.01e-14 ***\nheart_rate             0.020959   0.022136   0.947    0.346    \nage                   -0.004533   0.040201  -0.113    0.911    \nheight                -0.092808   0.185259  -0.501    0.618    \nweight                 0.137680   0.227125   0.606    0.546    \nethnicityWest African  0.607009   1.018407   0.596    0.553    \nethnicityWhite         0.381494   0.814760   0.468    0.641    \nsexmale               -0.843426   0.750917  -1.123    0.265    \nenergyyes              4.010409   0.879724   4.559 1.76e-05 ***\nBMI                   -0.519829   0.660626  -0.787    0.434    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.694 on 83 degrees of freedom\nMultiple R-squared:  0.8851,\tAdjusted R-squared:  0.8713 \nF-statistic: 63.95 on 10 and 83 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nOur model result above doesn’t indicate these issues present in our model, so multicollinearity is likely not an issue. To confirm this, let’s use a measure called the VIF (Variance Inflation Factor) computed as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 GVIF Df GVIF^(1/(2*Df))\nduration     3.523124  1        1.876999\nheart_rate   2.169643  1        1.472971\nage          2.406007  1        1.551131\nheight      32.491547  1        5.700136\nweight     113.782522  1       10.666889\nethnicity    2.585810  2        1.268087\nsex          1.685510  1        1.298272\nenergy       2.464164  1        1.569766\nBMI         52.942047  1        7.276129\n```\n\n\n:::\n:::\n\n\nThe VIF (Variance Inflation Factor) indicates that multicollinearity is present in our model. As a general guideline, a VIF value exceeding 10 is a cause for concern. In our case, height, weight, and BMI are contributing to this issue. Since BMI is derived from weight and height, we should include only one of these variables in our fitness model to address the multicollinearity problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula =  max_vo2 ~ duration + heart_rate + age + ethnicity + weight + sex + energy\nmodel3 <- lm(formula, data = excercise)\n```\n:::\n\n\nCheck the VIF again before interpreting the results. The model3 results indicated that multicollinearity is not an issue.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               GVIF Df GVIF^(1/(2*Df))\nduration   3.448220  1        1.856938\nheart_rate 2.122043  1        1.456723\nage        2.304269  1        1.517982\nethnicity  2.509935  2        1.258681\nweight     1.323586  1        1.150472\nsex        1.678684  1        1.295641\nenergy     2.462661  1        1.569287\n```\n\n\n:::\n:::\n\n\nExplore the final model results and interprete the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = formula, data = excercise)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.543 -1.428  0.310  1.485  5.285 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            8.451716   5.652625   1.495    0.139    \nduration               0.039455   0.004159   9.486 5.59e-15 ***\nheart_rate             0.024679   0.021822   1.131    0.261    \nage                   -0.006641   0.039216  -0.169    0.866    \nethnicityWest African  0.497364   1.009112   0.493    0.623    \nethnicityWhite         0.257835   0.805013   0.320    0.750    \nweight                -0.014511   0.024418  -0.594    0.554    \nsexmale               -0.847960   0.746998  -1.135    0.260    \nenergyyes              3.990363   0.876642   4.552 1.76e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.686 on 85 degrees of freedom\nMultiple R-squared:  0.8831,\tAdjusted R-squared:  0.8721 \nF-statistic: 80.26 on 8 and 85 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n##### **Model selection**\n\nThe process of selecting significant predictors for a model involves using criteria like R², Akaike Information Criteria (AIC), or Bayesian Information Criteria (BIC). There are various methods such as forward selection, backward selection, and stepwise selection, which are similar and yield comparable final models. Here, forward selection is used, which follows these steps:\n\n1.  Start with all predictors \"not in the model.\"\n\n2.  Iterate over these predictors, refitting the model with each variable.\n\n3.  Add the variable with the lowest p-value to the model if its p-value is less than α. Repeat until no more variables can be added.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the forward selection algorithm usign a alpha of 0.05\nmodel2 <- lm(lm(formula = max_vo2 ~., data = excercise))\n\nfwdmodel2 <- ols_step_forward_p(model2, penter=0.05)\nfwdmodel2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n                             Stepwise Summary                              \n-------------------------------------------------------------------------\nStep    Variable        AIC        SBC       SBIC        R2       Adj. R2 \n-------------------------------------------------------------------------\n 0      Base Model    648.809    653.896    378.795    0.00000    0.00000 \n 1      duration      488.802    496.432    220.812    0.82156    0.81962 \n 2      energy        455.051    465.224    188.681    0.87801    0.87533 \n 3      heart_rate    455.102    467.818    189.037    0.88051    0.87653 \n 4      sex           455.598    470.857    189.879    0.88241    0.87712 \n-------------------------------------------------------------------------\n\nFinal Model Output \n------------------\n\n                         Model Summary                          \n---------------------------------------------------------------\nR                       0.939       RMSE                 2.562 \nR-Squared               0.882       MSE                  6.930 \nAdj. R-Squared          0.877       Coef. Var            7.388 \nPred R-Squared          0.868       AIC                455.598 \nMAE                     1.949       SBC                470.857 \n---------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                ANOVA                                 \n---------------------------------------------------------------------\n                Sum of                                               \n               Squares        DF    Mean Square       F         Sig. \n---------------------------------------------------------------------\nRegression    4628.510         4       1157.128    166.966    0.0000 \nResidual       616.798        89          6.930                      \nTotal         5245.308        93                                     \n---------------------------------------------------------------------\n\n                                  Parameter Estimates                                    \n----------------------------------------------------------------------------------------\n      model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n----------------------------------------------------------------------------------------\n(Intercept)     6.416         3.005                  2.135    0.036     0.445    12.388 \n   duration     0.040         0.004        0.670    11.513    0.000     0.033     0.047 \n  energyyes     4.154         0.714        0.276     5.815    0.000     2.734     5.573 \n heart_rate     0.026         0.020        0.065     1.314    0.192    -0.013     0.066 \n    sexmale    -0.861         0.719       -0.055    -1.198    0.234    -2.289     0.567 \n----------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nVariable selection balances making the model both realistic and simple. Let's make a copy data set removing other variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexcercise <- excercise %>%dplyr:: select(max_vo2,duration,energy,sex)\n```\n:::\n\n\nLet's iterate the model with only duration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula2 =  max_vo2 ~ duration \nmodel4 <- lm(formula2, data = excercise)\nsummary(model4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = formula2, data = excercise)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8341 -1.7243  0.0338  1.8036  7.7629 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.06473    1.56876   2.591   0.0111 *  \nduration     0.05474    0.00266  20.581   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.19 on 92 degrees of freedom\nMultiple R-squared:  0.8216,\tAdjusted R-squared:  0.8196 \nF-statistic: 423.6 on 1 and 92 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nAdd energy: there is a small change in R\\^2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula3 =  max_vo2 ~ duration + energy     \nmodel5 <- lm(formula3, data = excercise)\nsummary(model5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = formula3, data = excercise)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2909 -1.4316  0.4028  1.7800  6.0130 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 8.109167   1.445456   5.610 2.16e-07 ***\nduration    0.044390   0.002727  16.281  < 2e-16 ***\nenergyyes   4.413022   0.680037   6.489 4.43e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.652 on 91 degrees of freedom\nMultiple R-squared:  0.878,\tAdjusted R-squared:  0.8753 \nF-statistic: 327.5 on 2 and 91 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nAdd sex: same for sex there is a small change from the previous model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula4 =  max_vo2 ~ duration + energy + sex    \nmodel6 <- lm(formula4, data = excercise)\n#summary(model4)\n# print estimated coefficient in a tidy model\nbroom::tidy(model6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   9.55     1.84         5.20 1.26e- 6\n2 duration      0.0431   0.00290     14.9  5.35e-26\n3 energyyes     4.12     0.717        5.75 1.22e- 7\n4 sexmale      -0.909    0.720       -1.26 2.10e- 1\n```\n\n\n:::\n:::\n\n\nCheck assumptions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(model6)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/assumption-1.png){width=672}\n:::\n:::\n\n\nwe can also plot one by one to have a better understanding of each output.\n\n***Linearity*** : the relationship between the response (max_vo2) and the regression is linear - a straight line through the fitted values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model6, which = 1)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/plot-1.png){width=672}\n:::\n:::\n\n\nNormality: the errors are distributed normally.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model6, which = 2)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nHomogeneity of variance: the error term *ε* has constant variance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model6, which = 3)\n```\n\n::: {.cell-output-display}\n![](idex_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n**Results**:\n\nIn *model4*, the F-statistic has a p-value of less than 2.2e-16, indicating strong statistical significance. This suggests that at least one predictor variable significantly affects the outcome. The r\\^2 result telling us the model has improved by 5% from model. To see the coefficient only result\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model6)$coefficient\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate  Std. Error   t value     Pr(>|t|)\n(Intercept)  9.54806984 1.837485216  5.196270 1.258410e-06\nduration     0.04312345 0.002897214 14.884456 5.346798e-26\nenergyyes    4.11926717 0.716703611  5.747518 1.221488e-07\nsexmale     -0.90907388 0.720484933 -1.261753 2.102980e-01\n```\n\n\n:::\n:::\n\n\n$$\nmax_vo2 = 6.416 + 0.04.duration(minutes) + 4.15.energy(yes) \n$$\n\nWe estimate that the Max_vo2 value will be 6.416 per minute per kilogram of body weight when both predictors (duration and energy) are zero.\n\nCoefficient for duration (0.04): Max_vo2 increases by 0.04 units for each additional minute of exercise.\n\nCoefficient for energy (4.15): Max_vo2 increases by 4.15 units if the energy condition is \"yes.\"\n\n::: callout-note\n#### **Model Diagnostics**\n\nWe check the model assumptions using various diagnostic plots and tests.\n\n1.  **Independence of Observations**: Durbin-Watson test\n\n2.  **Linearity**: Scatter plot of residuals vs. fitted values\n\n3.  **Homoscedasticity**: Fitted values vs. residuals plot\n\n4.  **Normality**: Histogram and Q-Q plot of residuals\n:::\n\n#### Train-Test Split and Model Performance\n\nHistorically the performance of the model was largely based on the goodness-of-fit test and assessment of residuals. For predictive modeling, perform cross-validation to ensure the model performs well with resampling data (loss function) - compare the predicted value to the actual value. The simplest approach involves:\n\n1.  Splitting the data into a training set and a test set.\n\n2.  Fitting the model to the training set.\n\n3.  Predicting responses in the test set.\n\n4.  Evaluating the prediction quality.\n\nIs there any rules for the percentage of classifying the train and test data ? There are [no specific rules](https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) for the percentage split between training and test data, but two common proportions are 80:20 and 70:30. The key is to balance maximizing the data for fitting the model while retaining enough data to assess its performance effectively.\n\nThere are different mechanism of splitting the data into train and test set. For demonstration, we will be covering the simple random sampling and the caret package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # for reproducibility\n\nn_train <- sample(1:nrow(excercise), round(nrow(excercise)*0.8))\n\ntrain_data <- excercise[n_train,]\n\ntest_data <- excercise[-n_train,]\n```\n:::\n\n\nUsing the `Caret` package.:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the data randomly into a training set and a test set. \nset.seed(100) \n\nn_train2 <- createDataPartition(excercise$max_vo2, p =0.8, list = FALSE) \n\n\ntrain_data2 <- excercise[n_train2, ] \n\ntest_data2 <- excercise[-n_train2, ]\n```\n:::\n\n\nMean Absolute Error (MAE), Mean Squared Error (MSE) along with the Root Mean Squared Error (RMSE) are the most common metrics to use.\n\nFit the model on the training data model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(max_vo2 ~ ., data = train_data2)\n\npredictions <- predict(model4, test_data2)\n```\n:::\n\n\nMeasure performance by comparing the prediction with the data using multiple criterion. The model with the smaller the value of RMSE and MAE is optimal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR_sq <- R2(predictions, test_data2$max_vo2)\nRMSE <- RMSE(predictions, test_data2$max_vo2)\nMSE <- MAE(predictions, test_data2$max_vo2)\n\nprint(c(R_sq, RMSE, MAE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] 0.8712019\n\n[[2]]\n[1] 2.966111\n\n[[3]]\nfunction (pred, obs, na.rm = FALSE) \nmean(abs(pred - obs), na.rm = na.rm)\n<bytecode: 0x0000028d5fb69d28>\n<environment: namespace:caret>\n```\n\n\n:::\n:::\n\n\nIt means on average, our prediction for max_vo2 (per minute per kilogram of body weight) is varied by \\~ 3 minutes/kg i.e., the prediction error is 8% .\n\nPrediction error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_error_rate <- RMSE/mean(test_data2$max_vo2)\npred_error_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.08370499\n```\n\n\n:::\n:::\n\n\n### Reference:\n\n1.  [A Step-By-Step Guide for Running a Complete Multiple Linear Regression Analysis in R](https://medium.com/analytics-vidhya/a-step-by-step-guide-for-running-a-complete-multiple-linear-regression-analysis-in-r-c08be169fe01)\n\n2.  [Stats and R](https://statsandr.com/blog/multiple-linear-regression-made-simple/)\n\n3.  [Multiple Linear Regression in R](http://sthda.com/english/articles/40-regression-analysis/168-multiple-linear-regression-in-r/)\n",
    "supporting": [
      "idex_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}